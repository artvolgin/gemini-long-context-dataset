{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "class ExtractLinksCS224n:\n",
    "    \"\"\"\n",
    "    A class to download and extract material links from the CS224n syllabus page.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Initialize the class with the syllabus URL.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL of the syllabus page.\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.base_url = self.construct_base_url()\n",
    "        self.html_content = None\n",
    "        self.soup = None\n",
    "        self.data = []\n",
    "\n",
    "    def construct_base_url(self):\n",
    "        \"\"\"\n",
    "        Construct the base URL from the provided URL for resolving relative links.\n",
    "\n",
    "        Returns:\n",
    "            str: The base URL.\n",
    "        \"\"\"\n",
    "        parsed_url = urlparse(self.url)\n",
    "        base = f\"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path.rsplit('/', 1)[0]}/\"\n",
    "        return base\n",
    "\n",
    "    def fetch_html(self):\n",
    "        \"\"\"\n",
    "        Fetch the HTML content from the syllabus URL.\n",
    "\n",
    "        Raises:\n",
    "            requests.HTTPError: If the HTTP request returned an unsuccessful status code.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(self.url)\n",
    "            response.raise_for_status()  # Raise an error for bad status\n",
    "            self.html_content = response.text\n",
    "            print(f\"Successfully fetched HTML content from {self.url}\")\n",
    "        except requests.HTTPError as http_err:\n",
    "            print(f\"HTTP error occurred while fetching HTML: {http_err}\")\n",
    "            raise\n",
    "        except Exception as err:\n",
    "            print(f\"An error occurred while fetching HTML: {err}\")\n",
    "            raise\n",
    "\n",
    "    def parse_html(self):\n",
    "        \"\"\"\n",
    "        Parse the fetched HTML content using BeautifulSoup.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If HTML content is empty.\n",
    "        \"\"\"\n",
    "        if not self.html_content:\n",
    "            raise ValueError(\"HTML content is empty. Please fetch the HTML first.\")\n",
    "        self.soup = BeautifulSoup(self.html_content, 'html.parser')\n",
    "        print(\"HTML content successfully parsed.\")\n",
    "\n",
    "    def determine_type(self, name, link):\n",
    "        \"\"\"\n",
    "        Determine the type of the link based on its name or link.\n",
    "\n",
    "        Args:\n",
    "            name (str): The text associated with the link.\n",
    "            link (str): The URL of the link.\n",
    "\n",
    "        Returns:\n",
    "            str: The category/type of the link.\n",
    "        \"\"\"\n",
    "        name_lower = name.lower()\n",
    "        if 'slide' in name_lower:\n",
    "            return 'slides'\n",
    "        elif 'note' in name_lower:\n",
    "            return 'notes'\n",
    "        elif 'code' in name_lower or 'colab' in name_lower:\n",
    "            return 'code'\n",
    "        elif 'handout' in name_lower or 'latex' in name_lower:\n",
    "            return 'handout'\n",
    "        elif 'assignment' in name_lower:\n",
    "            return 'assignment'\n",
    "        else:\n",
    "            return 'readings'\n",
    "\n",
    "    def process_links(self, cell, group, lecture_title, date_text):\n",
    "        \"\"\"\n",
    "        Process all links within a table cell.\n",
    "\n",
    "        Args:\n",
    "            cell (bs4.element.Tag): The table cell containing links.\n",
    "            group (str): The group/category of the links (e.g., 'description', 'course materials').\n",
    "            lecture_title (str): The title of the lecture associated with the links.\n",
    "            date_text (str): The date associated with the links.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dictionaries containing link information.\n",
    "        \"\"\"\n",
    "        links = []\n",
    "        for a in cell.find_all('a'):\n",
    "            link_text = a.get_text().strip('[]').strip()\n",
    "            link_href = a.get('href')\n",
    "            if link_href:\n",
    "                full_link = urljoin(self.base_url, link_href)\n",
    "                link_type = self.determine_type(link_text, link_href)\n",
    "                # Adjust 'name' if link_text is 'slides', 'notes', or 'code'\n",
    "                if link_text.lower() in ['slides', 'notes', 'code']:\n",
    "                    link_name = lecture_title\n",
    "                else:\n",
    "                    link_name = link_text\n",
    "                link_dict = {\n",
    "                    'link': full_link,\n",
    "                    'name': link_name,\n",
    "                    'date': date_text,\n",
    "                    'type': link_type,\n",
    "                    'group': group\n",
    "                }\n",
    "                links.append(link_dict)\n",
    "        return links\n",
    "\n",
    "    def extract_links(self):\n",
    "        \"\"\"\n",
    "        Extract all material links from the schedule table.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the schedule table cannot be found in the HTML.\n",
    "        \"\"\"\n",
    "        if not self.soup:\n",
    "            raise ValueError(\"Soup object is empty. Please parse the HTML first.\")\n",
    "\n",
    "        # Use CSS selector to find the table inside div#schedule\n",
    "        schedule_div = self.soup.find('div', id='schedule')\n",
    "        if not schedule_div:\n",
    "            raise ValueError(\"Could not find the schedule div in the HTML content.\")\n",
    "\n",
    "        schedule_table = schedule_div.find('table')\n",
    "        if not schedule_table:\n",
    "            raise ValueError(\"Could not find the schedule table in the HTML content.\")\n",
    "\n",
    "        print(\"Schedule table found. Beginning extraction of links...\")\n",
    "\n",
    "        # Iterate over the rows of the table\n",
    "        for row in schedule_table.find_all('tr'):\n",
    "            # Get all the cells in the row\n",
    "            cells = row.find_all('td')\n",
    "            if not cells:\n",
    "                continue  # Skip rows with no 'td's\n",
    "\n",
    "            # Assign cells based on position\n",
    "            date_cell = cells[0] if len(cells) > 0 else None\n",
    "            description_cell = cells[1] if len(cells) > 1 else None\n",
    "            materials_cell = cells[2] if len(cells) > 2 else None\n",
    "            events_cell = cells[3] if len(cells) > 3 else None\n",
    "            deadlines_cell = cells[4] if len(cells) > 4 else None\n",
    "\n",
    "            # Extract date\n",
    "            if date_cell:\n",
    "                date_lines = date_cell.get_text(separator='\\n').split('\\n')\n",
    "                date_line = date_lines[-1].strip() if date_lines else ''\n",
    "            else:\n",
    "                date_line = ''\n",
    "\n",
    "            # Extract the lecture title\n",
    "            if description_cell:\n",
    "                description_text = description_cell.get_text(separator='\\n').strip()\n",
    "                lecture_title = description_text.split('\\n')[0].strip()\n",
    "            else:\n",
    "                lecture_title = ''\n",
    "\n",
    "            # Process links in description_cell\n",
    "            if description_cell:\n",
    "                description_links = self.process_links(description_cell, 'description', lecture_title, date_line)\n",
    "                self.data.extend(description_links)\n",
    "\n",
    "            # Process links in materials_cell\n",
    "            if materials_cell:\n",
    "                materials_links = self.process_links(materials_cell, 'course materials', lecture_title, date_line)\n",
    "                self.data.extend(materials_links)\n",
    "\n",
    "            # Process links in events_cell\n",
    "            if events_cell:\n",
    "                events_links = self.process_links(events_cell, 'events', lecture_title, date_line)\n",
    "                self.data.extend(events_links)\n",
    "\n",
    "            # Process links in deadlines_cell\n",
    "            if deadlines_cell:\n",
    "                deadlines_links = self.process_links(deadlines_cell, 'events', lecture_title, date_line)\n",
    "                self.data.extend(deadlines_links)\n",
    "\n",
    "        print(f\"Extraction complete. Total links extracted: {len(self.data)}\")\n",
    "\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"\n",
    "        Retrieve the extracted link data.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dictionaries containing link information.\n",
    "        \"\"\"\n",
    "        return self.data\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        \"\"\"\n",
    "        Convert the extracted data into a pandas DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A pandas DataFrame containing the link data.\n",
    "        \"\"\"\n",
    "        if not self.data:\n",
    "            raise ValueError(\"No data available. Please run the extraction process first.\")\n",
    "        return pd.DataFrame(self.data)\n",
    "\n",
    "    def save_json(self, filename):\n",
    "        \"\"\"\n",
    "        Save the extracted link data to a JSON file.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The filename for the JSON output.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(self.data, f, indent=2)\n",
    "            print(f\"Data successfully saved to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving JSON: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Execute the full process of fetching, parsing, and extracting links.\n",
    "        \"\"\"\n",
    "        self.fetch_html()\n",
    "        self.parse_html()\n",
    "        self.extract_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, parse_qs, urlunparse\n",
    "import zipfile\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "class DownloadMaterials:\n",
    "    def __init__(self, download_dir='website_materials'):\n",
    "        \"\"\"\n",
    "        Initialize the DownloadMaterials class.\n",
    "\n",
    "        Args:\n",
    "            download_dir (str): The directory where downloaded files will be saved.\n",
    "        \"\"\"\n",
    "        self.download_dir = Path(download_dir)\n",
    "        self.download_dir.mkdir(exist_ok=True)\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "    @staticmethod\n",
    "    def url_to_filename(url):\n",
    "        \"\"\"\n",
    "        Convert a URL into a sanitized filename.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL to sanitize.\n",
    "\n",
    "        Returns:\n",
    "            str: A valid filename.\n",
    "        \"\"\"\n",
    "        url = re.sub(r'^https?:\\/\\/', '', url)  # Remove the protocol\n",
    "        filename = re.sub(r'[\\/:*?\"<>|]', '_', url)  # Replace unsafe characters\n",
    "        # filename = re.sub(r'[\\/:*?\"<>|].', '_', url)  # Replace unsafe characters\n",
    "        return filename[:255]  # Limit filename length to 255 characters\n",
    "\n",
    "    @staticmethod\n",
    "    def strip_extension(url):\n",
    "        \"\"\"\n",
    "        Remove the file extension from a URL's path.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL from which to remove the extension.\n",
    "\n",
    "        Returns:\n",
    "            str: The URL without the file extension.\n",
    "        \"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        path, ext = os.path.splitext(parsed.path)\n",
    "        # Reconstruct the URL without the extension\n",
    "        new_parsed = parsed._replace(path=path)\n",
    "        return urlunparse(new_parsed)\n",
    "\n",
    "    @staticmethod\n",
    "    def process_url(url):\n",
    "        \"\"\"\n",
    "        Process a URL to determine its file extension and adjust the URL if necessary.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL to process.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the processed URL and its file extension.\n",
    "        \"\"\"\n",
    "        parsed_url = urlparse(url)\n",
    "        netloc = parsed_url.netloc.lower()\n",
    "        path = parsed_url.path\n",
    "        query = parsed_url.query\n",
    "\n",
    "        # Special handling for arXiv 'abs' links\n",
    "        if 'arxiv.org' in netloc and '/abs/' in path:\n",
    "            paper_id = path.split('/abs/')[-1]\n",
    "            url = f'https://arxiv.org/pdf/{paper_id}.pdf'\n",
    "            ext = '.pdf'\n",
    "        # Special handling for Google Colab links\n",
    "        elif 'colab.research.google.com' in netloc:\n",
    "            notebook_id = None\n",
    "            if '/drive/' in path:\n",
    "                notebook_id = path.split('/drive/')[-1].split('/')[0]\n",
    "            elif 'id' in parse_qs(query):\n",
    "                notebook_id = parse_qs(query)['id'][0]\n",
    "            if notebook_id:\n",
    "                url = f'https://drive.google.com/uc?export=download&id={notebook_id}'\n",
    "                ext = '.ipynb'\n",
    "            else:\n",
    "                ext = '.html'\n",
    "        # Special handling for ACL Anthology links\n",
    "        elif 'aclanthology.org' in netloc or 'aclweb.org' in netloc:\n",
    "            # Handle URLs like 'http://www.aclweb.org/anthology/Q15-1016'\n",
    "            paper_id = path.strip('/').split('/')[-1]  # Extract 'Q15-1016'\n",
    "            url = f'https://aclanthology.org/{paper_id}.pdf'\n",
    "            ext = '.pdf'\n",
    "        else:\n",
    "            _, ext = os.path.splitext(path)\n",
    "            ext = ext.lower() if ext else '.html'\n",
    "        return url, ext\n",
    "\n",
    "    def download_file(self, download_url, file_name, ext, retries=1):\n",
    "        \"\"\"\n",
    "        Download a file from a URL.\n",
    "\n",
    "        Args:\n",
    "            download_url (str): The URL to download from.\n",
    "            file_name (str): The name to save the file as (without extension).\n",
    "            ext (str): The file extension.\n",
    "            retries (int): The number of retries in case of failure.\n",
    "\n",
    "        Returns:\n",
    "            Path or None: The path to the downloaded file or None if the download failed.\n",
    "        \"\"\"\n",
    "\n",
    "        # Correctly append the extension without duplicating it\n",
    "        file_path = self.download_dir / f\"{file_name}{ext}\"\n",
    "        if file_path.exists():\n",
    "            print(f\"File already exists: {file_path}\")\n",
    "            return file_path\n",
    "\n",
    "        # Determine if the download is from ACL Anthology\n",
    "        is_acl_anthology = 'aclanthology.org' in urlparse(download_url).netloc or 'aclweb.org' in urlparse(download_url).netloc\n",
    "\n",
    "        # Set specific headers for ACL Anthology to avoid 406 errors\n",
    "        headers = self.session.headers.copy()\n",
    "        if is_acl_anthology and ext == '.pdf':\n",
    "            headers.update({'Accept': 'application/pdf'})\n",
    "\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = self.session.get(download_url, stream=True, timeout=10, headers=headers)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                # Handle redirects\n",
    "                if response.history:\n",
    "                    final_url = response.url\n",
    "                    file_name = self.url_to_filename(self.strip_extension(final_url))\n",
    "                    _, final_ext = os.path.splitext(urlparse(final_url).path)\n",
    "                    final_ext = final_ext.lower() if final_ext else ext  # Fallback to original ext\n",
    "                    file_path = self.download_dir / f\"{file_name}{final_ext}\"\n",
    "                    ext = final_ext  # Update the extension\n",
    "\n",
    "                # Save HTML content if content type is text/html\n",
    "                content_type = response.headers.get('Content-Type', '')\n",
    "                if 'text/html' in content_type:\n",
    "                    html_path = file_path.with_suffix('.html')\n",
    "                    with open(html_path, 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "                    print(f\"Downloaded HTML instead of {ext} for URL: {download_url} as {html_path}\")\n",
    "                    return html_path  # Return the path to the HTML file\n",
    "\n",
    "                # Validate .ipynb content\n",
    "                if ext == '.ipynb':\n",
    "                    try:\n",
    "                        json_content = response.content.decode('utf-8')\n",
    "                        json.loads(json_content)  # Validate JSON\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Invalid JSON for URL: {download_url}\")\n",
    "                        return None\n",
    "                    \n",
    "                # Append .pdf suffix if the file is from arXiv\n",
    "                if 'arxiv' in file_name:\n",
    "                    print(ext)\n",
    "                    print(file_path)\n",
    "                    print(str(file_path) + '.pdf')\n",
    "                    file_path = (Path(str(file_path).replace('.', '_') + '.pdf'))\n",
    "\n",
    "                # Write the content to the file\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"Downloaded: {file_path}\")\n",
    "                return file_path\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Error downloading {download_url}: {e}. Retrying ({attempt + 1}/{retries})...\")\n",
    "                time.sleep(2)\n",
    "        print(f\"Failed to download {download_url} after {retries} attempts.\")\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_zip(file_path):\n",
    "        \"\"\"\n",
    "        Extract a ZIP file.\n",
    "\n",
    "        Args:\n",
    "            file_path (Path): The path to the ZIP file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                extract_path = file_path.parent / file_path.stem\n",
    "                extract_path.mkdir(exist_ok=True)\n",
    "                zip_ref.extractall(extract_path)\n",
    "            print(f\"Extracted ZIP: {file_path} to {extract_path}\")\n",
    "        except zipfile.BadZipFile as e:\n",
    "            print(f\"Failed to extract ZIP file {file_path}: {e}\")\n",
    "\n",
    "    def process_urls(self, urls):\n",
    "        \"\"\"\n",
    "        Process a list of URLs to prepare them for downloading.\n",
    "\n",
    "        Args:\n",
    "            urls (list): The list of URLs.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of dictionaries with processed URLs and metadata.\n",
    "        \"\"\"\n",
    "        processed_urls = []\n",
    "        for url in urls:\n",
    "            download_url, ext = self.process_url(url)\n",
    "            # Strip extension from the original URL to avoid duplication\n",
    "            url_no_ext = self.strip_extension(url)\n",
    "            file_name = self.url_to_filename(url_no_ext)\n",
    "            processed_urls.append({\n",
    "                'original_url': url,\n",
    "                'download_url': download_url,\n",
    "                'file_name': file_name,\n",
    "                'extension': ext\n",
    "            })\n",
    "        return processed_urls\n",
    "\n",
    "    def download_all_files(self, processed_urls):\n",
    "        \"\"\"\n",
    "        Download all files from a list of processed URLs.\n",
    "\n",
    "        Args:\n",
    "            processed_urls (list): A list of dictionaries with processed URLs and metadata.\n",
    "        \"\"\"\n",
    "\n",
    "        list_file_paths = []\n",
    "\n",
    "        for item in processed_urls:\n",
    "            print(f\"Downloading: {item['download_url']}\")\n",
    "            file_path = self.download_file(\n",
    "                download_url=item['download_url'],\n",
    "                file_name=item['file_name'],\n",
    "                ext=item['extension']\n",
    "            )\n",
    "            if file_path and file_path.suffix == '.zip':\n",
    "                self.extract_zip(file_path)\n",
    "            print()\n",
    "            time.sleep(1)\n",
    "            list_file_paths.append(str(file_path))\n",
    "\n",
    "        return list_file_paths\n",
    "\n",
    "    def merge_with_dataframe(self, df, processed_urls):\n",
    "        \"\"\"\n",
    "        Merge the processed URLs with the original DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The original DataFrame.\n",
    "            processed_urls (list): The processed URLs.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The merged DataFrame.\n",
    "        \"\"\"\n",
    "        processed_df = pd.DataFrame(processed_urls)\n",
    "        return df.merge(processed_df, left_on='link', right_on='original_url', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "from pathlib import Path\n",
    "\n",
    "class YouTubeSubtitlesDownloader:\n",
    "    \"\"\"\n",
    "    A class to download only subtitles from YouTube playlists using yt_dlp.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, playlist_url, download_path, subtitles_lang='en'):\n",
    "        \"\"\"\n",
    "        Initialize the downloader with playlist details and options.\n",
    "\n",
    "        Args:\n",
    "            playlist_url (str): URL of the YouTube playlist to download.\n",
    "            download_path (str): Path where subtitles will be downloaded.\n",
    "            subtitles_lang (str): Language for subtitles (default is 'en').\n",
    "        \"\"\"\n",
    "        self.playlist_url = playlist_url\n",
    "        self.download_path = Path(download_path)\n",
    "        self.download_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.subtitles_lang = subtitles_lang\n",
    "\n",
    "    def get_download_options(self):\n",
    "        \"\"\"\n",
    "        Generate yt_dlp options for downloading subtitles.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of yt_dlp options.\n",
    "        \"\"\"\n",
    "        options = {\n",
    "            'skip_download': True,  # Skip downloading video and audio\n",
    "            'writesubtitles': True,  # Enable subtitle download\n",
    "            'writeautomaticsub': True,  # Download auto-generated subtitles if no manual ones are available\n",
    "            'subtitleslangs': [self.subtitles_lang],  # Specify the language of subtitles\n",
    "            'outtmpl': str(self.download_path / '%(playlist_title)s/%(title)s.%(ext)s'),  # Save in organized folders\n",
    "        }\n",
    "        return options\n",
    "\n",
    "    def download_subtitles(self):\n",
    "        \"\"\"\n",
    "        Download the subtitles for the YouTube playlist.\n",
    "\n",
    "        Raises:\n",
    "            yt_dlp.utils.DownloadError: If there is an issue during the download.\n",
    "        \"\"\"\n",
    "        options = self.get_download_options()\n",
    "        with yt_dlp.YoutubeDL(options) as ydl:\n",
    "            ydl.download([self.playlist_url])\n",
    "        print(f\"Subtitles downloaded successfully to: {self.download_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "from pathlib import Path\n",
    "\n",
    "class YouTubePlaylistDownloader:\n",
    "    \"\"\"\n",
    "    A class to download YouTube playlists using yt_dlp with specified options.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, playlist_url, download_path, resolution='1080', subtitles=True, subtitles_lang='en', output_format='mp4'):\n",
    "        \"\"\"\n",
    "        Initialize the downloader with playlist details and options.\n",
    "\n",
    "        Args:\n",
    "            playlist_url (str): URL of the YouTube playlist to download.\n",
    "            download_path (str): Path where videos will be downloaded.\n",
    "            resolution (str): Desired video resolution (default is 1080p).\n",
    "            subtitles (bool): Whether to download subtitles (default is True).\n",
    "            subtitles_lang (str): Language for subtitles (default is 'en').\n",
    "            output_format (str): Output format for merged files (default is 'mp4').\n",
    "        \"\"\"\n",
    "        self.playlist_url = playlist_url\n",
    "        self.download_path = Path(download_path)\n",
    "        self.download_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.resolution = resolution\n",
    "        self.subtitles = subtitles\n",
    "        self.subtitles_lang = subtitles_lang\n",
    "        self.output_format = output_format\n",
    "\n",
    "    def get_download_options(self):\n",
    "        \"\"\"\n",
    "        Generate yt_dlp options based on the specified parameters.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of yt_dlp options.\n",
    "        \"\"\"\n",
    "        options = {\n",
    "            'format': f'bestvideo[height={self.resolution}]+bestaudio/best[height={self.resolution}]',\n",
    "            'outtmpl': str(self.download_path / '%(playlist_title)s/%(title)s.%(ext)s'),\n",
    "            'merge_output_format': self.output_format,\n",
    "        }\n",
    "        if self.subtitles:\n",
    "            options.update({\n",
    "                'subtitleslangs': [self.subtitles_lang],\n",
    "                'writesubtitles': True,\n",
    "                'writeautomaticsub': True,\n",
    "            })\n",
    "        return options\n",
    "\n",
    "    def download_playlist(self):\n",
    "        \"\"\"\n",
    "        Download the YouTube playlist with the configured options.\n",
    "\n",
    "        Raises:\n",
    "            yt_dlp.utils.DownloadError: If there is an issue during the download.\n",
    "        \"\"\"\n",
    "        options = self.get_download_options()\n",
    "        with yt_dlp.YoutubeDL(options) as ydl:\n",
    "            ydl.download([self.playlist_url])\n",
    "        print(f\"Playlist downloaded successfully to: {self.download_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CS224n - 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract links from html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the class with the syllabus URL\n",
    "links_extractor = ExtractLinksCS224n('https://web.stanford.edu/class/cs224n/index.html#schedule')\n",
    "\n",
    "# Run the extraction process\n",
    "links_extractor.run()\n",
    "\n",
    "# Retrieve the extracted data as a pandas DataFrame\n",
    "df_materials = links_extractor.get_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_selected_url(url: str) -> bool:\n",
    "    \"\"\"\n",
    "    Determines if a given URL matches any of the specified criteria:\n",
    "    \n",
    "    1) Contains 'stanford.edu' in the URL\n",
    "    2) Ends with '.pdf'\n",
    "    3) Contains 'arxiv' in the URL\n",
    "    4) Contains 'aclweb' in the URL\n",
    "    5) Contains 'colab.research.google' in the URL\n",
    "    6) Contains 'github.io' in the URL\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL to evaluate.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the URL matches any of the criteria, False otherwise.\n",
    "    \"\"\"\n",
    "    if not isinstance(url, str):\n",
    "        raise ValueError(\"Input must be a string representing a URL.\")\n",
    "    \n",
    "    # Convert the URL to lowercase for case-insensitive matching\n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    # Define the conditions\n",
    "    conditions = [\n",
    "        'stanford.edu' in url_lower,\n",
    "        url_lower.endswith('.pdf'),\n",
    "        'arxiv' in url_lower,\n",
    "        'aclweb' in url_lower,\n",
    "        'colab.research.google' in url_lower,\n",
    "        'github.io' in url_lower\n",
    "    ]\n",
    "    \n",
    "    # Debug: Print conditions (optional)\n",
    "    # print(f\"URL: {url}\")\n",
    "    # print(f\"Conditions: {conditions}\")\n",
    "    \n",
    "    # Return True if any condition is met\n",
    "    return any(conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_materials = df_materials[df_materials['link'].apply(is_selected_url)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download materials from links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials_downloader = DownloadMaterials(download_dir='cs224n-2024/website_materials')\n",
    "urls = df_materials['link'].tolist()\n",
    "processed_urls = materials_downloader.process_urls(urls)\n",
    "df_materials = materials_downloader.merge_with_dataframe(df_materials, processed_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files_paths = materials_downloader.download_all_files(df_materials.to_dict(orient='records'))\n",
    "df_materials['file_path'] = list_files_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expand df_materials with files in unziped folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_materials = df_materials[['link', 'name', 'date', 'type', 'group', 'file_path', 'extension']]\n",
    "df_materials = df_materials[df_materials['file_path'] != 'None']\n",
    "df_materials = df_materials.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def extract_zip_files(df, zip_column='file_path', root_dir='extracted_files'):\n",
    "    \"\"\"\n",
    "    Extract contents of ZIP files in the DataFrame and extend the DataFrame with their contents,\n",
    "    inheriting `date` and `link` from the ZIP file.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The original DataFrame containing ZIP file paths.\n",
    "        zip_column (str): The column in the DataFrame that contains ZIP file paths.\n",
    "        root_dir (str): Directory where ZIP files will be extracted.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Extended DataFrame including the contents of ZIP files.\n",
    "    \"\"\"\n",
    "    # Directory to extract ZIP files\n",
    "    root_path = Path(root_dir)\n",
    "    root_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # df_zip = df[df['extension'] == '.zip']\n",
    "    # df = df[df['extension'] != '.zip']\n",
    "\n",
    "    # List to collect rows for new files\n",
    "    new_rows = []\n",
    "\n",
    "    # Iterate through the DataFrame rows\n",
    "    for idx, row in df.iterrows():\n",
    "        file_path = Path(row[zip_column])\n",
    "\n",
    "        if file_path.suffix == '.zip' and file_path.exists():\n",
    "            try:\n",
    "                # Create a directory for this ZIP file\n",
    "                extract_dir = root_path / file_path.stem\n",
    "                extract_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # Extract the ZIP file\n",
    "                with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(extract_dir)\n",
    "\n",
    "                # Recursively collect file information\n",
    "                for extracted_file in extract_dir.rglob('*'):  # Recursively go through extracted files\n",
    "                    if extracted_file.is_file():  # Only process files\n",
    "                        new_rows.append({\n",
    "                            'link': row['link'],  # Inherit link from the parent ZIP file\n",
    "                            'name': extracted_file.name,\n",
    "                            'date': row['date'],  # Inherit date from the parent ZIP file\n",
    "                            'type': row['type'],  # Inherit type from the parent ZIP file\n",
    "                            'group': row['group'],  # Inherit group from the parent ZIP file\n",
    "                            'file_path': str(extracted_file),\n",
    "                            'extension': extracted_file.suffix\n",
    "                        })\n",
    "            except zipfile.BadZipFile as e:\n",
    "                print(f\"Failed to extract {file_path}: {e}\")\n",
    "\n",
    "    # Create a DataFrame for new rows\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "    # Remove zip files from the original DataFrame\n",
    "    df = df[df['extension'] != '.zip']\n",
    "\n",
    "    # Append new rows to the original DataFrame\n",
    "    extended_df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "    extended_df = extended_df[extended_df['extension'] != ''].reset_index(drop=True)\n",
    "\n",
    "    return extended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_materials = extract_zip_files(df_materials, zip_column='file_path', root_dir='/Users/artemvolgin/Repos/gemini-long/cs224n-2024/website_materials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_materials.to_pickle('cs224n-2024/df_materials.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download youtube videos and subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the playlist URL\n",
    "playlist_url = 'https://www.youtube.com/playlist?list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4'\n",
    "\n",
    "# Define the path for downloading\n",
    "download_path = 'cs224n-2024/youtube'\n",
    "\n",
    "# Initialize the downloader with desired options\n",
    "youtube_downloader = YouTubePlaylistDownloader(\n",
    "    playlist_url=playlist_url,\n",
    "    download_path=download_path,  # Specify the download path\n",
    "    resolution='1080',  # Download videos in 1080p resolution\n",
    "    subtitles=True,  # Enable subtitles\n",
    "    subtitles_lang='en',  # Subtitles language\n",
    "    output_format='mp4',  # Ensure merged output format is mp4\n",
    ")\n",
    "\n",
    "# Start the download\n",
    "youtube_downloader.download_playlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_playlist = 'cs224n-2024/youtube/Stanford CS224N： Natural Language Processing with Deep Learning ｜ 2023'\n",
    "list_youtube_paths = [path_to_playlist + '/' + x for x in os.listdir(path_to_playlist)]\n",
    "df_youtube = pd.DataFrame(list_youtube_paths, columns=['file_path'])\n",
    "df_youtube['extension'] = df_youtube['file_path'].apply(lambda x: Path(x).suffix)\n",
    "df_youtube['name'] = df_youtube['file_path'].apply(lambda x: Path(x).stem)\n",
    "df_youtube.to_pickle('cs224n-2024/df_youtube.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CS224n - 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract links from html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the class with the syllabus URL\n",
    "links_extractor = ExtractLinksCS224n('https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/index.html#schedule') \n",
    "links_extractor.run()\n",
    "df_materials = links_extractor.get_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download materials from links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials_downloader = DownloadMaterials(download_dir='cs224n-2019/website_materials')\n",
    "urls = df_materials['link'].tolist()\n",
    "processed_urls = materials_downloader.process_urls(urls)\n",
    "df_materials = materials_downloader.merge_with_dataframe(df_materials, processed_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> Select only the slides pdf <<<\n",
    "# df_materials = df_materials[(df_materials['type'] == 'slides') & (df_materials['extension'] == '.pdf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files_paths = materials_downloader.download_all_files(df_materials.to_dict(orient='records'))\n",
    "df_materials['file_path'] = list_files_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_materials = df_materials.reset_index(drop=True)\n",
    "df_materials = df_materials[['link', 'name', 'date', 'type', 'group', 'file_path', 'extension']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_materials.to_pickle('cs224n-2019/df_materials.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_materials = pd.read_pickle('cs224n-2019/df_materials.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube:tab] Extracting URL: https://www.youtube.com/watch?v=8rXD5-xhemo&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z\n",
      "[youtube:tab] Downloading playlist PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z - add --no-playlist to download just the video 8rXD5-xhemo\n",
      "[youtube:tab] PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z: Downloading webpage\n",
      "[youtube:tab] Extracting URL: https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z\n",
      "[youtube:tab] PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z: Downloading webpage\n",
      "[youtube:tab] PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z: Redownloading playlist API JSON with unavailable videos\n",
      "[download] Downloading playlist: Stanford CS224N: Natural Language Processing with Deep Learning Course | Winter 2019\n",
      "[youtube:tab] PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z page 1: Downloading API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube:tab] Incomplete data received. Retrying (1/3)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube:tab] PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z page 1: Downloading API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube:tab] Incomplete data received. Retrying (2/3)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube:tab] PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z page 1: Downloading API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube:tab] Incomplete data received. Retrying (3/3)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube:tab] PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z page 1: Downloading API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube:tab] Incomplete data received. Giving up after 3 retries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube:tab] Playlist Stanford CS224N: Natural Language Processing with Deep Learning Course | Winter 2019: Downloading 22 items of 22\n",
      "[download] Downloading item 1 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=8rXD5-xhemo\n",
      "[youtube] 8rXD5-xhemo: Downloading webpage\n",
      "[youtube] 8rXD5-xhemo: Downloading ios player API JSON\n",
      "[youtube] 8rXD5-xhemo: Downloading mweb player API JSON\n",
      "[youtube] 8rXD5-xhemo: Downloading m3u8 information\n",
      "[info] 8rXD5-xhemo: Downloading subtitles: en\n",
      "[info] 8rXD5-xhemo: Downloading 1 format(s): 137+251\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 1 – Introduction and Word Vectors.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 1 – Introduction and Word Vectors.en.vtt\n",
      "[download] 100% of  108.77KiB in 00:00:00 at 998.43KiB/s\n",
      "[download] Downloading item 2 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=kEMJRjEdNzM\n",
      "[youtube] kEMJRjEdNzM: Downloading webpage\n",
      "[youtube] kEMJRjEdNzM: Downloading ios player API JSON\n",
      "[youtube] kEMJRjEdNzM: Downloading mweb player API JSON\n",
      "[youtube] kEMJRjEdNzM: Downloading m3u8 information\n",
      "[info] kEMJRjEdNzM: Downloading subtitles: en\n",
      "[info] kEMJRjEdNzM: Downloading 1 format(s): 399+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 2 – Word Vectors and Word Senses.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 2 – Word Vectors and Word Senses.en.vtt\n",
      "[download] 100% of  109.07KiB in 00:00:00 at 1.67MiB/s\n",
      "[download] Downloading item 3 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=8CWyBNX6eDo\n",
      "[youtube] 8CWyBNX6eDo: Downloading webpage\n",
      "[youtube] 8CWyBNX6eDo: Downloading ios player API JSON\n",
      "[youtube] 8CWyBNX6eDo: Downloading mweb player API JSON\n",
      "[youtube] 8CWyBNX6eDo: Downloading m3u8 information\n",
      "[info] 8CWyBNX6eDo: Downloading subtitles: en\n",
      "[info] 8CWyBNX6eDo: Downloading 1 format(s): 399+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 3 – Neural Networks.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 3 – Neural Networks.en.vtt\n",
      "[download] 100% of   97.79KiB in 00:00:00 at 1.26MiB/s\n",
      "[download] Downloading item 4 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=yLYHDSv-288\n",
      "[youtube] yLYHDSv-288: Downloading webpage\n",
      "[youtube] yLYHDSv-288: Downloading ios player API JSON\n",
      "[youtube] yLYHDSv-288: Downloading mweb player API JSON\n",
      "[youtube] yLYHDSv-288: Downloading m3u8 information\n",
      "[info] yLYHDSv-288: Downloading subtitles: en\n",
      "[info] yLYHDSv-288: Downloading 1 format(s): 399+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 4 – Backpropagation.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 4 – Backpropagation.en.vtt\n",
      "[download] 100% of  109.65KiB in 00:00:00 at 1.35MiB/s\n",
      "[download] Downloading item 5 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=nC9_RfjYwqA\n",
      "[youtube] nC9_RfjYwqA: Downloading webpage\n",
      "[youtube] nC9_RfjYwqA: Downloading ios player API JSON\n",
      "[youtube] nC9_RfjYwqA: Downloading mweb player API JSON\n",
      "[youtube] nC9_RfjYwqA: Downloading m3u8 information\n",
      "[info] nC9_RfjYwqA: Downloading subtitles: en\n",
      "[info] nC9_RfjYwqA: Downloading 1 format(s): 399+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 5 – Dependency Parsing.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 5 – Dependency Parsing.en.vtt\n",
      "[download] 100% of  108.11KiB in 00:00:00 at 1.22MiB/s\n",
      "[download] Downloading item 6 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=iWea12EAu6U\n",
      "[youtube] iWea12EAu6U: Downloading webpage\n",
      "[youtube] iWea12EAu6U: Downloading ios player API JSON\n",
      "[youtube] iWea12EAu6U: Downloading mweb player API JSON\n",
      "[youtube] iWea12EAu6U: Downloading m3u8 information\n",
      "[info] iWea12EAu6U: Downloading subtitles: en\n",
      "[info] iWea12EAu6U: Downloading 1 format(s): 137+251\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 6 – Language Models and RNNs.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 6 – Language Models and RNNs.en.vtt\n",
      "[download] 100% of  112.71KiB in 00:00:00 at 830.51KiB/s\n",
      "[download] Downloading item 7 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=QEw0qEa0E50\n",
      "[youtube] QEw0qEa0E50: Downloading webpage\n",
      "[youtube] QEw0qEa0E50: Downloading ios player API JSON\n",
      "[youtube] QEw0qEa0E50: Downloading mweb player API JSON\n",
      "[youtube] QEw0qEa0E50: Downloading m3u8 information\n",
      "[info] QEw0qEa0E50: Downloading subtitles: en\n",
      "[info] QEw0qEa0E50: Downloading 1 format(s): 398+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 7 – Vanishing Gradients, Fancy RNNs.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 7 – Vanishing Gradients, Fancy RNNs.en.vtt\n",
      "[download] 100% of  117.87KiB in 00:00:00 at 790.10KiB/s\n",
      "[download] Downloading item 8 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=XXtpJxZBa2c\n",
      "[youtube] XXtpJxZBa2c: Downloading webpage\n",
      "[youtube] XXtpJxZBa2c: Downloading ios player API JSON\n",
      "[youtube] XXtpJxZBa2c: Downloading mweb player API JSON\n",
      "[youtube] XXtpJxZBa2c: Downloading m3u8 information\n",
      "[info] XXtpJxZBa2c: Downloading subtitles: en\n",
      "[info] XXtpJxZBa2c: Downloading 1 format(s): 399+251\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 8 – Translation, Seq2Seq, Attention.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 8 – Translation, Seq2Seq, Attention.en.vtt\n",
      "[download] 100% of  124.95KiB in 00:00:00 at 1.88MiB/s\n",
      "[download] Downloading item 9 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=fyqm8fRDgl0\n",
      "[youtube] fyqm8fRDgl0: Downloading webpage\n",
      "[youtube] fyqm8fRDgl0: Downloading ios player API JSON\n",
      "[youtube] fyqm8fRDgl0: Downloading mweb player API JSON\n",
      "[youtube] fyqm8fRDgl0: Downloading m3u8 information\n",
      "[info] fyqm8fRDgl0: Downloading subtitles: en\n",
      "[info] fyqm8fRDgl0: Downloading 1 format(s): 399+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 9 – Practical Tips for Projects.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 9 – Practical Tips for Projects.en.vtt\n",
      "[download] 100% of  117.31KiB in 00:00:00 at 987.07KiB/s\n",
      "[download] Downloading item 10 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=yIdF-17HwSk\n",
      "[youtube] yIdF-17HwSk: Downloading webpage\n",
      "[youtube] yIdF-17HwSk: Downloading ios player API JSON\n",
      "[youtube] yIdF-17HwSk: Downloading mweb player API JSON\n",
      "[youtube] yIdF-17HwSk: Downloading m3u8 information\n",
      "[info] yIdF-17HwSk: Downloading subtitles: en\n",
      "[info] yIdF-17HwSk: Downloading 1 format(s): 399+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 10 – Question Answering.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 10 – Question Answering.en.vtt\n",
      "[download] 100% of  108.87KiB in 00:00:00 at 1.41MiB/s\n",
      "[download] Downloading item 11 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=EAJoRA0KX7I\n",
      "[youtube] EAJoRA0KX7I: Downloading webpage\n",
      "[youtube] EAJoRA0KX7I: Downloading ios player API JSON\n",
      "[youtube] EAJoRA0KX7I: Downloading mweb player API JSON\n",
      "[youtube] EAJoRA0KX7I: Downloading m3u8 information\n",
      "[info] EAJoRA0KX7I: Downloading subtitles: en\n",
      "[info] EAJoRA0KX7I: Downloading 1 format(s): 399+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 11 – Convolutional Networks for NLP.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 11 – Convolutional Networks for NLP.en.vtt\n",
      "[download] 100% of  104.70KiB in 00:00:00 at 613.73KiB/s\n",
      "[download] Downloading item 12 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=9oTHFx0Gg3Q\n",
      "[youtube] 9oTHFx0Gg3Q: Downloading webpage\n",
      "[youtube] 9oTHFx0Gg3Q: Downloading ios player API JSON\n",
      "[youtube] 9oTHFx0Gg3Q: Downloading mweb player API JSON\n",
      "[youtube] 9oTHFx0Gg3Q: Downloading m3u8 information\n",
      "[info] 9oTHFx0Gg3Q: Downloading subtitles: en\n",
      "[info] 9oTHFx0Gg3Q: Downloading 1 format(s): 399+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 12 – Subword Models.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 12 – Subword Models.en.vtt\n",
      "[download] 100% of   99.23KiB in 00:00:00 at 670.80KiB/s\n",
      "[download] Downloading item 13 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=S-CspeZ8FHc\n",
      "[youtube] S-CspeZ8FHc: Downloading webpage\n",
      "[youtube] S-CspeZ8FHc: Downloading ios player API JSON\n",
      "[youtube] S-CspeZ8FHc: Downloading mweb player API JSON\n",
      "[youtube] S-CspeZ8FHc: Downloading m3u8 information\n",
      "[info] S-CspeZ8FHc: Downloading subtitles: en\n",
      "[info] S-CspeZ8FHc: Downloading 1 format(s): 399+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 13 – Contextual Word Embeddings.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 13 – Contextual Word Embeddings.en.vtt\n",
      "[download] 100% of  104.57KiB in 00:00:00 at 1.04MiB/s\n",
      "[download] Downloading item 14 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=5vcj8kSwBCY\n",
      "[youtube] 5vcj8kSwBCY: Downloading webpage\n",
      "[youtube] 5vcj8kSwBCY: Downloading ios player API JSON\n",
      "[youtube] 5vcj8kSwBCY: Downloading mweb player API JSON\n",
      "[youtube] 5vcj8kSwBCY: Downloading m3u8 information\n",
      "[info] 5vcj8kSwBCY: Downloading subtitles: en\n",
      "[info] 5vcj8kSwBCY: Downloading 1 format(s): 399+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 14 – Transformers and Self-Attention.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 14 – Transformers and Self-Attention.en.vtt\n",
      "[download] 100% of   87.24KiB in 00:00:00 at 1.26MiB/s\n",
      "[download] Downloading item 15 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=4uG1NMKNWCU\n",
      "[youtube] 4uG1NMKNWCU: Downloading webpage\n",
      "[youtube] 4uG1NMKNWCU: Downloading ios player API JSON\n",
      "[youtube] 4uG1NMKNWCU: Downloading mweb player API JSON\n",
      "[youtube] 4uG1NMKNWCU: Downloading m3u8 information\n",
      "[info] 4uG1NMKNWCU: Downloading subtitles: en\n",
      "[info] 4uG1NMKNWCU: Downloading 1 format(s): 399+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 15 – Natural Language Generation.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 15 – Natural Language Generation.en.vtt\n",
      "[download] 100% of  132.63KiB in 00:00:00 at 1.12MiB/s\n",
      "[download] Downloading item 16 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=i19m4GzBhfc\n",
      "[youtube] i19m4GzBhfc: Downloading webpage\n",
      "[youtube] i19m4GzBhfc: Downloading ios player API JSON\n",
      "[youtube] i19m4GzBhfc: Downloading mweb player API JSON\n",
      "[youtube] i19m4GzBhfc: Downloading m3u8 information\n",
      "[info] i19m4GzBhfc: Downloading subtitles: en\n",
      "[info] i19m4GzBhfc: Downloading 1 format(s): 399+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 16 – Coreference Resolution.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 16 – Coreference Resolution.en.vtt\n",
      "[download] 100% of  106.42KiB in 00:00:00 at 1.12MiB/s\n",
      "[download] Downloading item 17 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=M8dsZsEtEsg\n",
      "[youtube] M8dsZsEtEsg: Downloading webpage\n",
      "[youtube] M8dsZsEtEsg: Downloading ios player API JSON\n",
      "[youtube] M8dsZsEtEsg: Downloading mweb player API JSON\n",
      "[youtube] M8dsZsEtEsg: Downloading m3u8 information\n",
      "[info] M8dsZsEtEsg: Downloading subtitles: en\n",
      "[info] M8dsZsEtEsg: Downloading 1 format(s): 399+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 17 – Multitask Learning.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 17 – Multitask Learning.en.vtt\n",
      "[download] 100% of  118.61KiB in 00:00:00 at 1.03MiB/s\n",
      "[download] Downloading item 18 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=6Z4A3RSf-HY\n",
      "[youtube] 6Z4A3RSf-HY: Downloading webpage\n",
      "[youtube] 6Z4A3RSf-HY: Downloading ios player API JSON\n",
      "[youtube] 6Z4A3RSf-HY: Downloading mweb player API JSON\n",
      "[youtube] 6Z4A3RSf-HY: Downloading m3u8 information\n",
      "[info] 6Z4A3RSf-HY: Downloading subtitles: en\n",
      "[info] 6Z4A3RSf-HY: Downloading 1 format(s): 399+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 18 – Constituency Parsing, TreeRNNs.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 18 – Constituency Parsing, TreeRNNs.en.vtt\n",
      "[download] 100% of  108.11KiB in 00:00:00 at 563.87KiB/s\n",
      "[download] Downloading item 19 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=XR8YSRcuVLE\n",
      "[youtube] XR8YSRcuVLE: Downloading webpage\n",
      "[youtube] XR8YSRcuVLE: Downloading ios player API JSON\n",
      "[youtube] XR8YSRcuVLE: Downloading mweb player API JSON\n",
      "[youtube] XR8YSRcuVLE: Downloading m3u8 information\n",
      "[info] XR8YSRcuVLE: Downloading subtitles: en\n",
      "[info] XR8YSRcuVLE: Downloading 1 format(s): 137+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 19 – Bias in AI.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 19 – Bias in AI.en.vtt\n",
      "[download] 100% of   86.42KiB in 00:00:00 at 1.37MiB/s\n",
      "[download] Downloading item 20 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=3wWZBGN-iX8\n",
      "[youtube] 3wWZBGN-iX8: Downloading webpage\n",
      "[youtube] 3wWZBGN-iX8: Downloading ios player API JSON\n",
      "[youtube] 3wWZBGN-iX8: Downloading mweb player API JSON\n",
      "[youtube] 3wWZBGN-iX8: Downloading m3u8 information\n",
      "[info] 3wWZBGN-iX8: Downloading subtitles: en\n",
      "[info] 3wWZBGN-iX8: Downloading 1 format(s): 399+251\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 20 – Future of NLP + Deep Learning.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2019 ｜ Lecture 20 – Future of NLP + Deep Learning.en.vtt\n",
      "[download] 100% of  117.03KiB in 00:00:00 at 930.31KiB/s\n",
      "[download] Downloading item 21 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=mp95Z5yM92c\n",
      "[youtube] mp95Z5yM92c: Downloading webpage\n",
      "[youtube] mp95Z5yM92c: Downloading ios player API JSON\n",
      "[youtube] mp95Z5yM92c: Downloading mweb player API JSON\n",
      "[youtube] mp95Z5yM92c: Downloading m3u8 information\n",
      "[info] mp95Z5yM92c: Downloading subtitles: en\n",
      "[info] mp95Z5yM92c: Downloading 1 format(s): 137+251\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ Low Resource Machine Translation.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ Low Resource Machine Translation.en.vtt\n",
      "[download] 100% of  563.95KiB in 00:00:00 at 2.98MiB/s\n",
      "[download] Downloading item 22 of 22\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=knTc-NQSjKA\n",
      "[youtube] knTc-NQSjKA: Downloading webpage\n",
      "[youtube] knTc-NQSjKA: Downloading ios player API JSON\n",
      "[youtube] knTc-NQSjKA: Downloading mweb player API JSON\n",
      "[youtube] knTc-NQSjKA: Downloading m3u8 information\n",
      "[info] knTc-NQSjKA: Downloading subtitles: en\n",
      "[info] knTc-NQSjKA: Downloading 1 format(s): 137+251\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ BERT and Other Pre-trained Language Models.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019/Stanford CS224N： NLP with Deep Learning ｜ Winter 2020 ｜ BERT and Other Pre-trained Language Models.en.vtt\n",
      "[download] 100% of  559.61KiB in 00:00:00 at 3.42MiB/s\n",
      "[download] Finished downloading playlist: Stanford CS224N: Natural Language Processing with Deep Learning Course | Winter 2019\n",
      "Subtitles downloaded successfully to: /Users/artemvolgin/Repos/gemini-long/cs224n-2019/youtube\n"
     ]
    }
   ],
   "source": [
    "# Define the playlist URL\n",
    "playlist_url = 'https://www.youtube.com/watch?v=8rXD5-xhemo&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z'\n",
    "\n",
    "# Define the path for downloading\n",
    "download_path = 'cs224n-2019/youtube'\n",
    "\n",
    "# Initialize the downloader with desired options\n",
    "youtube_downloader = YouTubeSubtitlesDownloader(\n",
    "    playlist_url=playlist_url,\n",
    "    download_path=download_path,  # Specify the download path\n",
    "    subtitles_lang='en',  # Subtitles language\n",
    ")\n",
    "\n",
    "# Start the download\n",
    "youtube_downloader.download_subtitles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_playlist = 'cs224n-2019/youtube/Stanford CS224N： Natural Language Processing with Deep Learning Course ｜ Winter 2019'\n",
    "list_youtube_paths = [path_to_playlist + '/' + x for x in os.listdir(path_to_playlist)]\n",
    "df_youtube = pd.DataFrame(list_youtube_paths, columns=['file_path'])\n",
    "df_youtube['extension'] = df_youtube['file_path'].apply(lambda x: Path(x).suffix)\n",
    "df_youtube['name'] = df_youtube['file_path'].apply(lambda x: Path(x).stem)\n",
    "df_youtube.to_pickle('cs224n-2019/df_youtube.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CS231n - 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "class ExtractLinksCS231n:\n",
    "    \"\"\"\n",
    "    A class to download and extract material links from a webpage.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Initialize the class with the syllabus URL.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL of the syllabus page.\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.base_url = self.construct_base_url()\n",
    "        self.html_content = None\n",
    "        self.soup = None\n",
    "        self.data = []\n",
    "\n",
    "    def construct_base_url(self):\n",
    "        \"\"\"\n",
    "        Construct the base URL from the provided URL for resolving relative links.\n",
    "\n",
    "        Returns:\n",
    "            str: The base URL.\n",
    "        \"\"\"\n",
    "        parsed_url = urlparse(self.url)\n",
    "        base = f\"{parsed_url.scheme}://{parsed_url.netloc}/\"\n",
    "        return base\n",
    "\n",
    "    def fetch_html(self):\n",
    "        \"\"\"\n",
    "        Fetch the HTML content from the syllabus URL.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(self.url)\n",
    "            response.raise_for_status()  # Raise an error for bad status\n",
    "            self.html_content = response.text\n",
    "            print(f\"Successfully fetched HTML content from {self.url}\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"An error occurred while fetching HTML: {e}\")\n",
    "            raise\n",
    "\n",
    "    def parse_html(self):\n",
    "        \"\"\"\n",
    "        Parse the fetched HTML content using BeautifulSoup.\n",
    "        \"\"\"\n",
    "        if not self.html_content:\n",
    "            raise ValueError(\"HTML content is empty. Please fetch the HTML first.\")\n",
    "        self.soup = BeautifulSoup(self.html_content, 'html.parser')\n",
    "        print(\"HTML content successfully parsed.\")\n",
    "\n",
    "    def determine_type(self, name, link):\n",
    "        \"\"\"\n",
    "        Determine the type of the link based on its name or link.\n",
    "        \"\"\"\n",
    "        name_lower = name.lower()\n",
    "        if 'slide' in name_lower:\n",
    "            return 'slides'\n",
    "        elif 'note' in name_lower:\n",
    "            return 'notes'\n",
    "        elif 'code' in name_lower:\n",
    "            return 'code'\n",
    "        elif 'handout' in name_lower:\n",
    "            return 'handout'\n",
    "        elif 'assignment' in name_lower:\n",
    "            return 'assignment'\n",
    "        else:\n",
    "            return 'readings'\n",
    "\n",
    "    def find_schedule_table(self):\n",
    "        \"\"\"\n",
    "        Find the schedule table on the webpage, considering potential changes in structure.\n",
    "        \"\"\"\n",
    "        # First, look for the div with id 'schedule'\n",
    "        schedule_div = self.soup.find('div', id='schedule')\n",
    "        if schedule_div:\n",
    "            return schedule_div.find('table')\n",
    "\n",
    "        # If not found, try finding a table with specific classes (based on the observed structure)\n",
    "        schedule_table = self.soup.find('table', class_='table')\n",
    "        if schedule_table:\n",
    "            return schedule_table\n",
    "\n",
    "        # Fall back to searching for the first table\n",
    "        return self.soup.find('table')\n",
    "\n",
    "    def process_links(self, cell, group, lecture_title, date_text):\n",
    "        \"\"\"\n",
    "        Process all links within a table cell.\n",
    "        \"\"\"\n",
    "        links = []\n",
    "        for a in cell.find_all('a'):\n",
    "            link_text = a.get_text().strip('[]').strip()\n",
    "            link_href = a.get('href')\n",
    "            if link_href:\n",
    "                full_link = urljoin(self.base_url, link_href)\n",
    "                link_type = self.determine_type(link_text, link_href)\n",
    "                link_dict = {\n",
    "                    'link': full_link,\n",
    "                    'name': link_text if link_text.lower() not in ['slides', 'notes', 'code'] else lecture_title,\n",
    "                    'date': date_text,\n",
    "                    'type': link_type,\n",
    "                    'group': group\n",
    "                }\n",
    "                links.append(link_dict)\n",
    "        return links\n",
    "\n",
    "    def extract_links(self):\n",
    "        \"\"\"\n",
    "        Extract all material links from the schedule table.\n",
    "        \"\"\"\n",
    "        if not self.soup:\n",
    "            raise ValueError(\"Soup object is empty. Please parse the HTML first.\")\n",
    "\n",
    "        # Locate the schedule table\n",
    "        schedule_table = self.find_schedule_table()\n",
    "        if not schedule_table:\n",
    "            raise ValueError(\"Could not find the schedule table in the HTML content.\")\n",
    "\n",
    "        print(\"Schedule table found. Beginning extraction of links...\")\n",
    "\n",
    "        # Iterate over the rows of the table\n",
    "        for row in schedule_table.find_all('tr'):\n",
    "            cells = row.find_all('td')\n",
    "            if not cells:\n",
    "                continue  # Skip rows with no 'td's\n",
    "\n",
    "            # Assign cells based on position\n",
    "            date_cell = cells[0] if len(cells) > 0 else None\n",
    "            description_cell = cells[1] if len(cells) > 1 else None\n",
    "            materials_cell = cells[2] if len(cells) > 2 else None\n",
    "\n",
    "            # Extract date\n",
    "            date_text = date_cell.get_text(strip=True) if date_cell else ''\n",
    "\n",
    "            # Extract the lecture title\n",
    "            lecture_title = description_cell.get_text(strip=True).split('\\n')[0] if description_cell else ''\n",
    "\n",
    "            # Process links in description_cell\n",
    "            if description_cell:\n",
    "                description_links = self.process_links(description_cell, 'description', lecture_title, date_text)\n",
    "                self.data.extend(description_links)\n",
    "\n",
    "            # Process links in materials_cell\n",
    "            if materials_cell:\n",
    "                materials_links = self.process_links(materials_cell, 'course materials', lecture_title, date_text)\n",
    "                self.data.extend(materials_links)\n",
    "\n",
    "        print(f\"Extraction complete. Total links extracted: {len(self.data)}\")\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        \"\"\"\n",
    "        Convert the extracted data into a pandas DataFrame.\n",
    "        \"\"\"\n",
    "        if not self.data:\n",
    "            raise ValueError(\"No data available. Please run the extraction process first.\")\n",
    "        return pd.DataFrame(self.data)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Execute the full process of fetching, parsing, and extracting links.\n",
    "        \"\"\"\n",
    "        self.fetch_html()\n",
    "        self.parse_html()\n",
    "        self.extract_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the class with the syllabus URL\n",
    "links_extractor = ExtractLinksCS231n('https://cs231n.stanford.edu/schedule.html') \n",
    "links_extractor.run()\n",
    "df_materials = links_extractor.get_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download materials from links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "materials_downloader = DownloadMaterials(download_dir='cs231n-2024/website_materials')\n",
    "urls = df_materials['link'].tolist()\n",
    "processed_urls = materials_downloader.process_urls(urls)\n",
    "df_materials = materials_downloader.merge_with_dataframe(df_materials, processed_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> Select only the slides pdf <<<\n",
    "# df_materials = df_materials[(df_materials['type'] == 'slides') & (df_materials['extension'] == '.pdf')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files_paths = materials_downloader.download_all_files(df_materials.to_dict(orient='records'))\n",
    "df_materials['file_path'] = list_files_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_materials = df_materials[['link', 'name', 'date', 'type', 'group', 'file_path', 'extension']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_materials.to_pickle('cs231n-2024/df_materials.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_materials = pd.read_pickle('cs231n-2024/df_materials.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube:tab] Extracting URL: https://www.youtube.com/playlist?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk\n",
      "[youtube:tab] PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk: Downloading webpage\n",
      "[youtube:tab] PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk: Redownloading playlist API JSON with unavailable videos\n",
      "[download] Downloading playlist: Stanford University CS231n, Spring 2017\n",
      "[youtube:tab] PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk page 1: Downloading API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube:tab] Incomplete data received. Retrying (1/3)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube:tab] PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk page 1: Downloading API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube:tab] Incomplete data received. Retrying (2/3)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube:tab] PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk page 1: Downloading API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube:tab] Incomplete data received. Retrying (3/3)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube:tab] PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk page 1: Downloading API JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [youtube:tab] Incomplete data received. Giving up after 3 retries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube:tab] Playlist Stanford University CS231n, Spring 2017: Downloading 16 items of 16\n",
      "[download] Downloading item 1 of 16\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=vT1JzLTH4G4\n",
      "[youtube] vT1JzLTH4G4: Downloading webpage\n",
      "[youtube] vT1JzLTH4G4: Downloading ios player API JSON\n",
      "[youtube] vT1JzLTH4G4: Downloading mweb player API JSON\n",
      "[youtube] vT1JzLTH4G4: Downloading m3u8 information\n",
      "[info] vT1JzLTH4G4: Downloading subtitles: en\n",
      "[info] vT1JzLTH4G4: Downloading 1 format(s): 616+251\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 1 ｜ Introduction to Convolutional Neural Networks for Visual Recognition.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 1 ｜ Introduction to Convolutional Neural Networks for Visual Recognition.en.vtt\n",
      "[download] 100% of   75.83KiB in 00:00:00 at 1.37MiB/s\n",
      "[download] Downloading item 2 of 16\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=OoUX-nOEjG0\n",
      "[youtube] OoUX-nOEjG0: Downloading webpage\n",
      "[youtube] OoUX-nOEjG0: Downloading ios player API JSON\n",
      "[youtube] OoUX-nOEjG0: Downloading mweb player API JSON\n",
      "[youtube] OoUX-nOEjG0: Downloading m3u8 information\n",
      "[info] OoUX-nOEjG0: Downloading subtitles: en\n",
      "[info] OoUX-nOEjG0: Downloading 1 format(s): 399+251\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 2 ｜ Image Classification.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 2 ｜ Image Classification.en.vtt\n",
      "[download] 100% of  100.81KiB in 00:00:00 at 710.18KiB/s\n",
      "[download] Downloading item 3 of 16\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=h7iBpEHGVNc\n",
      "[youtube] h7iBpEHGVNc: Downloading webpage\n",
      "[youtube] h7iBpEHGVNc: Downloading ios player API JSON\n",
      "[youtube] h7iBpEHGVNc: Downloading mweb player API JSON\n",
      "[youtube] h7iBpEHGVNc: Downloading m3u8 information\n",
      "[info] h7iBpEHGVNc: Downloading subtitles: en\n",
      "[info] h7iBpEHGVNc: Downloading 1 format(s): 399+251\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 3 ｜ Loss Functions and Optimization.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 3 ｜ Loss Functions and Optimization.en.vtt\n",
      "[download] 100% of  125.67KiB in 00:00:00 at 1.68MiB/s\n",
      "[download] Downloading item 4 of 16\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=d14TUNcbn1k\n",
      "[youtube] d14TUNcbn1k: Downloading webpage\n",
      "[youtube] d14TUNcbn1k: Downloading ios player API JSON\n",
      "[youtube] d14TUNcbn1k: Downloading mweb player API JSON\n",
      "[youtube] d14TUNcbn1k: Downloading m3u8 information\n",
      "[info] d14TUNcbn1k: Downloading subtitles: en\n",
      "[info] d14TUNcbn1k: Downloading 1 format(s): 399+251\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 4 ｜ Introduction to Neural Networks.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 4 ｜ Introduction to Neural Networks.en.vtt\n",
      "[download] 100% of  107.89KiB in 00:00:00 at 1.33MiB/s\n",
      "[download] Downloading item 5 of 16\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=bNb2fEVKeEo\n",
      "[youtube] bNb2fEVKeEo: Downloading webpage\n",
      "[youtube] bNb2fEVKeEo: Downloading ios player API JSON\n",
      "[youtube] bNb2fEVKeEo: Downloading mweb player API JSON\n",
      "[youtube] bNb2fEVKeEo: Downloading m3u8 information\n",
      "[info] bNb2fEVKeEo: Downloading subtitles: en\n",
      "[info] bNb2fEVKeEo: Downloading 1 format(s): 399+251\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 5 ｜ Convolutional Neural Networks.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 5 ｜ Convolutional Neural Networks.en.vtt\n",
      "[download] 100% of  100.44KiB in 00:00:00 at 1.52MiB/s\n",
      "[download] Downloading item 6 of 16\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=wEoyxE0GP2M\n",
      "[youtube] wEoyxE0GP2M: Downloading webpage\n",
      "[youtube] wEoyxE0GP2M: Downloading ios player API JSON\n",
      "[youtube] wEoyxE0GP2M: Downloading mweb player API JSON\n",
      "[youtube] wEoyxE0GP2M: Downloading m3u8 information\n",
      "[info] wEoyxE0GP2M: Downloading subtitles: en\n",
      "[info] wEoyxE0GP2M: Downloading 1 format(s): 399+251\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 6 ｜ Training Neural Networks I.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 6 ｜ Training Neural Networks I.en.vtt\n",
      "[download] 100% of  119.65KiB in 00:00:00 at 1.04MiB/s\n",
      "[download] Downloading item 7 of 16\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=_JB0AO7QxSA\n",
      "[youtube] _JB0AO7QxSA: Downloading webpage\n",
      "[youtube] _JB0AO7QxSA: Downloading ios player API JSON\n",
      "[youtube] _JB0AO7QxSA: Downloading mweb player API JSON\n",
      "[youtube] _JB0AO7QxSA: Downloading m3u8 information\n",
      "[info] _JB0AO7QxSA: Downloading subtitles: en\n",
      "[info] _JB0AO7QxSA: Downloading 1 format(s): 137+251\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 7 ｜ Training Neural Networks II.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 7 ｜ Training Neural Networks II.en.vtt\n",
      "[download] 100% of  134.07KiB in 00:00:00 at 1.35MiB/s\n",
      "[download] Downloading item 8 of 16\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=6SlgtELqOWc\n",
      "[youtube] 6SlgtELqOWc: Downloading webpage\n",
      "[youtube] 6SlgtELqOWc: Downloading ios player API JSON\n",
      "[youtube] 6SlgtELqOWc: Downloading mweb player API JSON\n",
      "[youtube] 6SlgtELqOWc: Downloading m3u8 information\n",
      "[info] 6SlgtELqOWc: Downloading subtitles: en\n",
      "[info] 6SlgtELqOWc: Downloading 1 format(s): 399+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 8 ｜ Deep Learning Software.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 8 ｜ Deep Learning Software.en.vtt\n",
      "[download] 100% of  137.92KiB in 00:00:00 at 1.73MiB/s\n",
      "[download] Downloading item 9 of 16\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=DAOcjicFr1Y\n",
      "[youtube] DAOcjicFr1Y: Downloading webpage\n",
      "[youtube] DAOcjicFr1Y: Downloading ios player API JSON\n",
      "[youtube] DAOcjicFr1Y: Downloading mweb player API JSON\n",
      "[youtube] DAOcjicFr1Y: Downloading m3u8 information\n",
      "[info] DAOcjicFr1Y: Downloading subtitles: en\n",
      "[info] DAOcjicFr1Y: Downloading 1 format(s): 399+251\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 9 ｜ CNN Architectures.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 9 ｜ CNN Architectures.en.vtt\n",
      "[download] 100% of  108.96KiB in 00:00:00 at 1.19MiB/s\n",
      "[download] Downloading item 10 of 16\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=6niqTuYFZLQ\n",
      "[youtube] 6niqTuYFZLQ: Downloading webpage\n",
      "[youtube] 6niqTuYFZLQ: Downloading ios player API JSON\n",
      "[youtube] 6niqTuYFZLQ: Downloading mweb player API JSON\n",
      "[youtube] 6niqTuYFZLQ: Downloading m3u8 information\n",
      "[info] 6niqTuYFZLQ: Downloading subtitles: en\n",
      "[info] 6niqTuYFZLQ: Downloading 1 format(s): 399+251\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 10 ｜ Recurrent Neural Networks.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 10 ｜ Recurrent Neural Networks.en.vtt\n",
      "[download] 100% of  139.19KiB in 00:00:00 at 1.80MiB/s\n",
      "[download] Downloading item 11 of 16\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=nDPWywWRIRo\n",
      "[youtube] nDPWywWRIRo: Downloading webpage\n",
      "[youtube] nDPWywWRIRo: Downloading ios player API JSON\n",
      "[youtube] nDPWywWRIRo: Downloading mweb player API JSON\n",
      "[youtube] nDPWywWRIRo: Downloading m3u8 information\n",
      "[info] nDPWywWRIRo: Downloading subtitles: en\n",
      "[info] nDPWywWRIRo: Downloading 1 format(s): 399+251\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 11 ｜ Detection and Segmentation.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 11 ｜ Detection and Segmentation.en.vtt\n",
      "[download] 100% of  124.94KiB in 00:00:00 at 1.67MiB/s\n",
      "[download] Downloading item 12 of 16\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=6wcs6szJWMY\n",
      "[youtube] 6wcs6szJWMY: Downloading webpage\n",
      "[youtube] 6wcs6szJWMY: Downloading ios player API JSON\n",
      "[youtube] 6wcs6szJWMY: Downloading mweb player API JSON\n",
      "[youtube] 6wcs6szJWMY: Downloading m3u8 information\n",
      "[info] 6wcs6szJWMY: Downloading subtitles: en\n",
      "[info] 6wcs6szJWMY: Downloading 1 format(s): 137+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 12 ｜ Visualizing and Understanding.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 12 ｜ Visualizing and Understanding.en.vtt\n",
      "[download] 100% of  136.66KiB in 00:00:00 at 1.69MiB/s\n",
      "[download] Downloading item 13 of 16\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=5WoItGTWV54\n",
      "[youtube] 5WoItGTWV54: Downloading webpage\n",
      "[youtube] 5WoItGTWV54: Downloading ios player API JSON\n",
      "[youtube] 5WoItGTWV54: Downloading mweb player API JSON\n",
      "[youtube] 5WoItGTWV54: Downloading m3u8 information\n",
      "[info] 5WoItGTWV54: Downloading subtitles: en\n",
      "[info] 5WoItGTWV54: Downloading 1 format(s): 399+251\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 13 ｜ Generative Models.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 13 ｜ Generative Models.en.vtt\n",
      "[download] 100% of  109.47KiB in 00:00:00 at 1.03MiB/s\n",
      "[download] Downloading item 14 of 16\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=lvoHnicueoE\n",
      "[youtube] lvoHnicueoE: Downloading webpage\n",
      "[youtube] lvoHnicueoE: Downloading ios player API JSON\n",
      "[youtube] lvoHnicueoE: Downloading mweb player API JSON\n",
      "[youtube] lvoHnicueoE: Downloading m3u8 information\n",
      "[info] lvoHnicueoE: Downloading subtitles: en\n",
      "[info] lvoHnicueoE: Downloading 1 format(s): 399+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 14 ｜ Deep Reinforcement Learning.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 14 ｜ Deep Reinforcement Learning.en.vtt\n",
      "[download] 100% of   98.31KiB in 00:00:00 at 1.60MiB/s\n",
      "[download] Downloading item 15 of 16\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=eZdOkDtYMoo\n",
      "[youtube] eZdOkDtYMoo: Downloading webpage\n",
      "[youtube] eZdOkDtYMoo: Downloading ios player API JSON\n",
      "[youtube] eZdOkDtYMoo: Downloading mweb player API JSON\n",
      "[youtube] eZdOkDtYMoo: Downloading m3u8 information\n",
      "[info] eZdOkDtYMoo: Downloading subtitles: en\n",
      "[info] eZdOkDtYMoo: Downloading 1 format(s): 399+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 15 ｜ Efficient Methods and Hardware for Deep Learning.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 15 ｜ Efficient Methods and Hardware for Deep Learning.en.vtt\n",
      "[download] 100% of   89.58KiB in 00:00:00 at 1.49MiB/s\n",
      "[download] Downloading item 16 of 16\n",
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=CIfsB_EYsVI\n",
      "[youtube] CIfsB_EYsVI: Downloading webpage\n",
      "[youtube] CIfsB_EYsVI: Downloading ios player API JSON\n",
      "[youtube] CIfsB_EYsVI: Downloading mweb player API JSON\n",
      "[youtube] CIfsB_EYsVI: Downloading m3u8 information\n",
      "[info] CIfsB_EYsVI: Downloading subtitles: en\n",
      "[info] CIfsB_EYsVI: Downloading 1 format(s): 399+140\n",
      "[info] Writing video subtitles to: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 16 ｜ Adversarial Examples and Adversarial Training.en.vtt\n",
      "[download] Destination: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube/Stanford University CS231n, Spring 2017/Lecture 16 ｜ Adversarial Examples and Adversarial Training.en.vtt\n",
      "[download] 100% of  131.74KiB in 00:00:00 at 1.62MiB/s\n",
      "[download] Finished downloading playlist: Stanford University CS231n, Spring 2017\n",
      "Subtitles downloaded successfully to: /Users/artemvolgin/Repos/gemini-long/cs231n-2024/youtube\n"
     ]
    }
   ],
   "source": [
    "# Define the playlist URL\n",
    "playlist_url = 'https://www.youtube.com/playlist?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk'\n",
    "\n",
    "# Define the path for downloading\n",
    "download_path = 'cs231n-2024/youtube'\n",
    "\n",
    "# Initialize the downloader with desired options\n",
    "youtube_downloader = YouTubeSubtitlesDownloader(\n",
    "    playlist_url=playlist_url,\n",
    "    download_path=download_path,  # Specify the download path\n",
    "    subtitles_lang='en',  # Subtitles language\n",
    ")\n",
    "\n",
    "# Start the download\n",
    "youtube_downloader.download_subtitles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_playlist = 'cs231n-2024/youtube/Stanford University CS231n, Spring 2017'\n",
    "list_youtube_paths = [path_to_playlist + '/' + x for x in os.listdir(path_to_playlist)]\n",
    "df_youtube = pd.DataFrame(list_youtube_paths, columns=['file_path'])\n",
    "df_youtube['extension'] = df_youtube['file_path'].apply(lambda x: Path(x).suffix)\n",
    "df_youtube['name'] = df_youtube['file_path'].apply(lambda x: Path(x).stem)\n",
    "df_youtube.to_pickle('cs231n-2024/df_youtube.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
